{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting using spatio-temporal data with combined Graph Convolution + LSTM model\n",
    "\n",
    "SOURCE: https://stellargraph.readthedocs.io/en/stable/demos/time-series/gcn-lstm-time-series.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of many real-world phenomena are spatio-temporal in nature. Traffic forecasting is a quintessential example of spatio-temporal problems for which we present here a deep learning framework that models speed prediction using spatio-temporal data. The task is challenging due to two main inter-linked factors: (1) the complex spatial dependency on road networks, and (2) non-linear temporal dynamics with changing road conditions.\n",
    "\n",
    "To address these challenges, here we  explore a neural network architecture that learns from both the spatial road network data and time-series of historical speed changes to forecast speeds on road segments at a future time. In the following we demo how to forecast speeds on road segments through a `graph convolution` and `LSTM` hybrid model.  The spatial dependency of the road networks are learnt through multiple graph convolution layers  stacked over multiple LSTM,  sequence to sequence model, layers that leverage the historical speeds on top of the network structure to predicts speeds in the future for each entity. \n",
    "\n",
    "The architecture of the GCN-LSTM model is inspired by the paper: [T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction](https://ieeexplore.ieee.org/document/8809901).\n",
    "\n",
    "The authors have made available the implementation of their model in their GitHub [repository](https://github.com/lehaifeng/T-GCN).\n",
    "There has been a few differences in the architecture proposed in the paper and the implementation of the graph convolution component, these issues have been documented [here](https://github.com/lehaifeng/T-GCN/issues/18) and [here](https://github.com/lehaifeng/T-GCN/issues/14). The `GCN_LSTM` model in `StellarGraph`  emulates the model as explained in the paper while giving additional flexibility of adding any number of `graph convolution` and `LSTM` layers. \n",
    "\n",
    "Concretely, the architecture of `GCN_LSTM` is as follows:\n",
    "\n",
    "1. User defined number of  graph convolutional layers (Reference: [Kipf & Welling (ICLR 2017)](http://arxiv.org/abs/1609.02907)).\n",
    "2. User defined number of  LSTM layers. The [TGCN](https://ieeexplore.ieee.org/document/8809901) uses GRU instead of LSTM. In practice there are not any remarkable differences between the two types of layers. We use LSTM as they are more frequently used.\n",
    "3. A Dropout and a Dense layer as they experimentally showed improvement in performance and managing over-fitting.\n",
    "\n",
    "## References: \n",
    "\n",
    "* [T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction](https://ieeexplore.ieee.org/document/8809901)\n",
    "* [https://github.com/lehaifeng/T-GCN](https://github.com/lehaifeng/T-GCN)\n",
    "* [Semi-Supervised Classification with Graph Convolutional Networks](http://arxiv.org/abs/1609.02907)\n",
    "\n",
    "**Note: this method is applicable for uni-variate timeseries forecasting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "VersionCheck"
    ]
   },
   "outputs": [],
   "source": [
    "# verify that we're using the correct version of StellarGraph for this notebook\n",
    "import stellargraph as sg\n",
    "\n",
    "try:\n",
    "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
    "except AttributeError:\n",
    "    raise ValueError(\n",
    "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
    "    ) from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We apply the GCN-LSTM model to the **Los-loop** data. This traffic dataset\n",
    "contains traffic information collected from loop detectors in the highway of Los Angeles County (Jagadish\n",
    "et al., 2014).  There are several processed versions of this dataset used by the research community working in Traffic forecasting space. \n",
    "\n",
    "This demo is based on the preprocessed version of the dataset used by the TGCN paper. It can be directly accessed from there [github repo](https://github.com/lehaifeng/T-GCN/tree/master/data). \n",
    "\n",
    "This dataset  contains traffic speeds from Mar.1 to Mar.7, 2012 of 207 sensors, recorded every 5 minutes. \n",
    "\n",
    "In order to use the model, we need:\n",
    "\n",
    "* A N by N adjacency matrix, which describes the distance relationship between the N sensors,\n",
    "* A N by T feature matrix, which describes the (f_1, .., f_T) speed records over T timesteps for the N sensors.\n",
    "\n",
    "A couple of other references for the same data albeit different time length are as follows: \n",
    "\n",
    "* [DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING](https://github.com/liyaguang/DCRNN/tree/master/data): This dataset consists of 207 sensors and collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment. It has some missing values.\n",
    "* [ST-MetaNet: Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning](https://github.com/panzheyi/ST-MetaNet/tree/master/traffic-prediction). This work uses the DCRNN preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo is based on the preprocessed version of the dataset used by the TGCN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sg.datasets.METR_LA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DataLoadingLinks"
    ]
   },
   "source": [
    "(See [the \"Loading from Pandas\" demo](../basics/loading-pandas.ipynb) for details on how data can be loaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DataLoading"
    ]
   },
   "outputs": [],
   "source": [
    "speed_data, sensor_dist_adj = dataset.load()\n",
    "num_nodes, time_len = speed_data.shape\n",
    "print(\"No. of sensors:\", num_nodes, \"\\nNo of timesteps:\", time_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at a sample of speed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dist_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, there are 2016 observations (timesteps) of speed records over 207 sensors. Speeds are recorded every 5 minutes.  This means that, for a single hour, you will have 12 observations. Similarly, a single day will contain 288 (12x24) observations. Overall, the data consists of speeds recorded every 5 minutes over 207  for 7 days (12X24X7).\n",
    "\n",
    "### Forecasting with spatio-temporal data as a supervised learning problem \n",
    "\n",
    "Time series forecasting problem can be cast as a supervised learning problem. We can do this by using previous timesteps as input features and use the next timestep as the output to predict. Then, the spatio-temporal forecasting question can be modeled as predicting the feature value in the future, given the historical values of the feature for that entity  as well as the feature values of the entities \"connected\" to the entity.  For example, the speed prediction problem, the historical speeds of the sensors are the timeseries and the distance between the sensors is the indicator for connectivity or closeness of sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "Just like for modeling any standard supervised learning problem, we first split the data into mutually exclusive train and test sets. However, unlike, a standard supervised learning problem,  in timeseries analysis, the data is in some chronological time respecting order and the train/test happens along the timeline. Lets say, we use the first `T_t` observations for training and the remaining `T - T_t` of the  total `T` observations for testing.  \n",
    "\n",
    "In the following we use first 80% observations for training and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_portion):\n",
    "    time_len = data.shape[1]\n",
    "    train_size = int(time_len * train_portion)\n",
    "    train_data = np.array(data.iloc[:, :train_size])\n",
    "    test_data = np.array(data.iloc[:, train_size:])\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(speed_data, train_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "It is generally a good practice to  rescale the data from the original range so that all values are within the range of 0 and 1. Normalization can be useful and even necessary when your time series data has input values with differing scales.  In the following we normalize the speed timeseries by the maximum and minimum values of speeds in the train data. \n",
    "\n",
    "Note: `MinMaxScaler` in `scikit learn` library is typically used for transforming data. However, in timeseries data since the features are distinct timesteps, so using the historical range of values in a particular timestep as the range of values in later timesteps, may not be correct.  Hence, we use the maximum and the minimum of the entire range of values in the timeseries to scale and transform the train and test sets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_data, test_data):\n",
    "    max_speed = train_data.max()\n",
    "    min_speed = train_data.min()\n",
    "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
    "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, test_scaled = scale_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence data preparation for LSTM\n",
    "\n",
    "We first need to prepare the data to be fed into an LSTM. \n",
    "The LSTM model learns a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn.\n",
    "\n",
    "To make it concrete in terms of the speed prediction problem, we choose to use 50 minutes of historical speed observations to predict the speed in future, lets say, 1 hour ahead. Hence, we would first  reshape the timeseries data into windows of 10 historical observations for each segment as the input and the speed 60 minutes later is the label we are interested in predicting. We use the sliding window approach to prepare the data. This is how it works:  \n",
    "\n",
    "* Starting from the beginning of the timeseries, we take the first 10 speed records as the 10 input features and the speed 12 timesteps head (60 minutes) as the speed we want to predict. \n",
    "* Shift the timeseries by one timestep and take the 10 observations from the current point as the input features and the speed one hour ahead as the output to predict. \n",
    "* Keep shifting by 1 timestep and picking the 10 timestep window from the current time as input feature and the speed one hour ahead of the 10th timestep as the output to predict, for the entire data.\n",
    "* The above steps are done for each sensor. \n",
    "\n",
    "The function below returns the above transformed timeseries data for the model to train on. The parameter `seq_len` is the size of the past window of information. The `pre_len` is how far in the future does the model need to learn to predict. \n",
    "\n",
    "For this demo: \n",
    "\n",
    "* Each training observation are 10 historical speeds (`seq_len`).\n",
    "* Each training prediction is the speed 60 minutes later (`pre_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "pre_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_data_preparation(seq_len, pre_len, train_data, test_data):\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "\n",
    "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        a = train_data[:, i : i + seq_len + pre_len]\n",
    "        trainX.append(a[:, :seq_len])\n",
    "        trainY.append(a[:, -1])\n",
    "\n",
    "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = test_data[:, i : i + seq_len + pre_len]\n",
    "        testX.append(b[:, :seq_len])\n",
    "        testY.append(b[:, -1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    testX = np.array(testX)\n",
    "    testY = np.array(testY)\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StellarGraph Graph Convolution and LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.layer import GCN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=sensor_dist_adj,\n",
    "    gc_layer_sizes=[16, 10],\n",
    "    gc_activations=[\"relu\", \"relu\"],\n",
    "    lstm_layer_sizes=[200, 200],\n",
    "    lstm_activations=[\"tanh\", \"tanh\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input, x_output = gcn_lstm.in_out_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=100,\n",
    "    batch_size=60,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    validation_data=[testX, testY],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train loss: \",\n",
    "    history.history[\"loss\"][-1],\n",
    "    \"\\nTest loss:\",\n",
    "    history.history[\"val_loss\"][-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ythat = model.predict(trainX)\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale values\n",
    "\n",
    "Rescale the predicted values to the original value range of the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale values\n",
    "max_speed = train_data.max()\n",
    "min_speed = train_data.min()\n",
    "\n",
    "## actual train and test values\n",
    "train_rescref = np.array(trainY * max_speed)\n",
    "test_rescref = np.array(testY * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale model predicted values\n",
    "train_rescpred = np.array((ythat) * max_speed)\n",
    "test_rescpred = np.array((yhat) * max_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the performance of the model\n",
    "\n",
    "To understand how well the model is performing, we compare it against a naive benchmark.\n",
    "\n",
    "1. Naive prediction: using the most recently **observed** value as the predicted value. Note, that albeit being **naive** this is a very strong baseline to beat. Especially, when speeds are recorded at a 5 minutes granularity,  one does not expect many drastic changes within such a short period of time. Hence, for short-term predictions naive is a reasonable good guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive prediction benchmark (using latest observed value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive prediction benchmark (using previous observed value)\n",
    "\n",
    "testnpred = np.array(testX)[\n",
    "    :, :, -1\n",
    "]  # picking the last speed of the 10 sequence for each segment in each sample\n",
    "testnpredc = (testnpred) * max_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance measures\n",
    "\n",
    "seg_mael = []\n",
    "seg_masel = []\n",
    "seg_nmael = []\n",
    "\n",
    "for j in range(testX.shape[-1]):\n",
    "\n",
    "    seg_mael.append(\n",
    "        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))\n",
    "    )  # Mean Absolute Error for NN\n",
    "    seg_nmael.append(\n",
    "        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))\n",
    "    )  # Mean Absolute Error for naive prediction\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(\n",
    "            seg_mael[-1] / seg_nmael[-1]\n",
    "        )  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\n",
    "    \"Total (ave) MASE for per-segment NN/naive MAE: \"\n",
    "    + str(np.nanmean(np.array(seg_masel)))\n",
    ")\n",
    "print(\n",
    "    \"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot violin plot of MAE for naive and NN predictions\n",
    "fig, ax = plt.subplots()\n",
    "# xl = minsl\n",
    "\n",
    "ax.violinplot(\n",
    "    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0\n",
    ")\n",
    "\n",
    "ax.violinplot(\n",
    "    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0\n",
    ")\n",
    "\n",
    "line1 = mlines.Line2D([], [], label=\"NN\")\n",
    "line2 = mlines.Line2D([], [], color=\"C1\", label=\"Instantaneous\")\n",
    "\n",
    "ax.set_xlabel(\"Scaled distribution amplitude (after Gaussian convolution)\")\n",
    "ax.set_ylabel(\"Mean Absolute Error\")\n",
    "ax.set_title(\"Distribution over segments: NN pred (blue) and naive pred (orange)\")\n",
    "plt.legend(handles=(line1, line2), title=\"Prediction Model\", loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of actual and predicted speeds on a sample sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##all test result visualization\n",
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "a_pred = test_rescpred[:, 100]\n",
    "a_true = test_rescref[:, 100]\n",
    "plt.plot(a_pred, \"r-\", label=\"prediction\")\n",
    "plt.plot(a_true, \"b-\", label=\"true\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
